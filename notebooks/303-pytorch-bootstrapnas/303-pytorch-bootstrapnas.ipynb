{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "git68adWeq4l"
   },
   "source": [
    "# Automated Neural Architecture Search with BootstrapNAS\n",
    "This notebook demonstrates how to use [BootstrapNAS](https://arxiv.org/abs/2112.10878), a capability in NNCF to generate weight-sharing super-networks from pre-trained models. Once the super-network has been generated, BootstrapNAS can train it and search for efficient sub-networks. \n",
    "\n",
    "We will use [MobileNet-V2](https://arxiv.org/abs/1801.04381) pre-trained with CIFAR-10. MobileNet-V2 is an efficient mobile architecture based on inverted residual blocks. Our goal is to discover alternative models, a.k.a., subnetworks, that are more efficient than the input pre-trained model.\n",
    "\n",
    "\\*BootstrapNAS is an **experimental feature** in NNCF.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/jpablomch/bootstrapnas/raw/main/architecture.png\" alt=\"BootstrapNAS Architecture\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "BootstrapNAS (1) takes as input a pre-trained model. (2) It uses this model to generate a weight-sharing super-network. (3) BootstrapNAS then applies a training strategy, and once the super-network has been trained, (4) it searches for efficient subnetworks that satisfy the user's requirements. (5) The configuration of the discovered sub-network(s) is returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M1xndNu-z_2"
   },
   "source": [
    "## Imports and Settings\n",
    "\n",
    "Import NNCF and all auxiliary packages to your Python code.\n",
    "Set a name for the model, and the image width and height that will be used for the network. Also define paths where the output files will be stored. \n",
    "\n",
    "> NOTE: All NNCF logging messages below ERROR level (INFO and WARNING) are disabled to simplify the tutorial. For production use, it is recommended to enable logging, by removing ```set_log_level(logging.ERROR)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BtaM_i2mEB0z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import warnings  # to disable warnings on export to ONNX\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import torch\n",
    "import nncf  # Important - should be imported directly after torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from bootstrapnas_utils import MobileNetV2, validate, train_epoch\n",
    "\n",
    "from nncf.common.utils.logger import set_log_level\n",
    "set_log_level(logging.ERROR)  # Disables all NNCF info and warning messages\n",
    "\n",
    "from nncf import NNCFConfig\n",
    "from nncf.config.structures import BNAdaptationInitArgs\n",
    "from nncf.experimental.torch.nas.bootstrapNAS import EpochBasedTrainingAlgorithm\n",
    "from nncf.experimental.torch.nas.bootstrapNAS import SearchAlgorithm\n",
    "from nncf.torch import create_compressed_model, register_default_init_args\n",
    "from nncf.torch.initialization import wrap_dataloader_for_init\n",
    "from nncf.torch.model_creation import create_nncf_network\n",
    "\n",
    "from openvino.runtime import Core\n",
    "from torch.jit import TracerWarning\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "MODEL_DIR = Path(\"model\")\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "DATA_DIR = Path(\"data\")\n",
    "BASE_MODEL_NAME = \"mobilenet-V2\"\n",
    "image_size = 32\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Paths where models will be stored\n",
    "fp32_pth_path = Path(MODEL_DIR / (BASE_MODEL_NAME + \"_fp32\")).with_suffix(\".pth\")\n",
    "model_onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME )).with_suffix(\".onnx\")\n",
    "supernet_onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_supernet\")).with_suffix(\".onnx\")\n",
    "subnet_onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_subnet\")).with_suffix(\".onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pre-trained model weights\n",
    "Download the pre-trained weights for MobileNet-V2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'model/mobilenet-V2_fp32.pth' already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jpmunoz/AutoML/openvino_notebooks/notebooks/303-pytorch-bootstrapnas/model/mobilenet-V2_fp32.pth')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's possible to train FP32 model from scratch, but it might be slow. So the pre-trained weights are downloaded by default.\n",
    "pretrained_on_cifar10 = True\n",
    "fp32_pth_url = \"http://hsw1.jf.intel.com/share/bootstrapNAS/checkpoints/cifar10/mobilenet_v2.pt\"\n",
    "download_file(fp32_pth_url, directory=MODEL_DIR, filename=fp32_pth_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIo5S145S0Ug",
    "outputId": "9a2db892-eb38-4863-dfdb-560aa12c8232",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare CIFAR-10 dataset\n",
    "Next, prepare the CIFAR-10 dataset. The CIFAR-10 dataset contains:\n",
    "* 60,000 images of shape 3x32x32\n",
    "* 10 different classes (6,000 images per class): airplane, automobile, etc. \n",
    "\n",
    "Here, the dataloader is created for both the training and validation dataset which includes normalization, crop, and other transformation.  Each dataloader uses 4 workers and a batch size of 64 for training and 1000 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-HxsU71bEbLS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = DATA_DIR / \"cifar10\"\n",
    "\n",
    "image_size = 32\n",
    "size = int(image_size / 0.875)\n",
    "normalize = transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                                         std=(0.2471, 0.2435, 0.2616))\n",
    "list_val_transforms = [\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ]\n",
    "val_transform = transforms.Compose(list_val_transforms)\n",
    "\n",
    "list_train_transforms = [\n",
    "            transforms.RandomCrop(image_size, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ]\n",
    "    \n",
    "train_transform = transforms.Compose(list_train_transforms)\n",
    "    \n",
    "download = False \n",
    "if not DATASET_DIR.exists(): \n",
    "    download = True\n",
    "\n",
    "train_dataset = datasets.CIFAR10(DATASET_DIR, train=True, transform=train_transform, download=download)\n",
    "val_dataset = datasets.CIFAR10(DATASET_DIR, train=False, transform=val_transform, download=download)\n",
    "\n",
    "batch_size_val = 1000\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "pin_memory = device != 'cpu'\n",
    "val_sampler = torch.utils.data.SequentialSampler(val_dataset) \n",
    "train_sampler = None\n",
    "train_shuffle = None\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size_val, shuffle=False,\n",
    "        num_workers=workers, pin_memory=pin_memory,\n",
    "        sampler=val_sampler, drop_last=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=train_shuffle,\n",
    "            num_workers=workers, pin_memory=pin_memory, sampler=train_sampler, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZX2GAh3W7ZT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generate a super-network from a pre-trained model\n",
    " \n",
    "Using NNCF for model compression assumes that the user has a pre-trained model and a training pipeline. Next, we demonstrate one possible training pipeline.\n",
    "\n",
    "### Evaluate pre-trained model\n",
    "Load the pretrained model and evaluate using validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 0/10]\tTime 0.616 (0.616)\tLoss 0.229 (0.229)\tAcc@1 94.10 (94.10)\tAcc@5 99.80 (99.80)\n",
      " * Acc@1 93.910 Acc@5 99.830\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV2()\n",
    "state_dict = torch.load(fp32_pth_path)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_top1_acc, _, _ = validate(model, val_loader, criterion) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input pre-trained model has a Top 1 accuracy of **93.91%**. We will use BootstrapNAS to find a sub-network with similar accuracy but more efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train supernetwork\n",
    "We use the pre-trained MobileNet-V2 model to generate a weight-sharing super-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[0][  0/781]\tTime 0.441 (0.441)\tLoss 0.143 (0.143)\tAcc@1 93.75 (93.75)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 0/10]\tTime 0.483 (0.483)\tLoss 1.258 (1.258)\tAcc@1 60.40 (60.40)\tAcc@5 91.90 (91.90)\n",
      " * Acc@1 61.470 Acc@5 92.530\n",
      "Test: [ 0/10]\tTime 0.528 (0.528)\tLoss 0.272 (0.272)\tAcc@1 92.20 (92.20)\tAcc@5 99.90 (99.90)\n",
      " * Acc@1 92.150 Acc@5 99.810\n",
      "Epoch:[1][  0/781]\tTime 0.399 (0.399)\tLoss 0.122 (0.122)\tAcc@1 96.88 (96.88)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 0/10]\tTime 0.486 (0.486)\tLoss 0.924 (0.924)\tAcc@1 72.30 (72.30)\tAcc@5 95.90 (95.90)\n",
      " * Acc@1 73.180 Acc@5 96.330\n",
      "Test: [ 0/10]\tTime 0.534 (0.534)\tLoss 0.439 (0.439)\tAcc@1 87.70 (87.70)\tAcc@5 99.10 (99.10)\n",
      " * Acc@1 87.440 Acc@5 99.260\n",
      "Epoch:[2][  0/781]\tTime 0.379 (0.379)\tLoss 0.425 (0.425)\tAcc@1 89.06 (89.06)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 0/10]\tTime 0.495 (0.495)\tLoss 0.942 (0.942)\tAcc@1 70.70 (70.70)\tAcc@5 96.00 (96.00)\n",
      " * Acc@1 71.300 Acc@5 96.100\n",
      "Test: [ 0/10]\tTime 0.547 (0.547)\tLoss 0.381 (0.381)\tAcc@1 88.60 (88.60)\tAcc@5 99.40 (99.40)\n",
      " * Acc@1 89.300 Acc@5 99.530\n",
      "Epoch:[3][  0/781]\tTime 0.377 (0.377)\tLoss 0.325 (0.325)\tAcc@1 90.62 (90.62)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 0/10]\tTime 0.542 (0.542)\tLoss 1.006 (1.006)\tAcc@1 69.40 (69.40)\tAcc@5 95.40 (95.40)\n",
      " * Acc@1 70.180 Acc@5 95.620\n",
      "Test: [ 0/10]\tTime 0.571 (0.571)\tLoss 0.374 (0.374)\tAcc@1 89.20 (89.20)\tAcc@5 99.40 (99.40)\n",
      " * Acc@1 89.640 Acc@5 99.600\n",
      "Epoch:[4][  0/781]\tTime 0.438 (0.438)\tLoss 0.468 (0.468)\tAcc@1 85.94 (85.94)\tAcc@5 100.00 (100.00)\n",
      "Test: [ 0/10]\tTime 0.506 (0.506)\tLoss 0.878 (0.878)\tAcc@1 74.00 (74.00)\tAcc@5 96.70 (96.70)\n",
      " * Acc@1 75.340 Acc@5 96.960\n",
      "Test: [ 0/10]\tTime 0.544 (0.544)\tLoss 0.549 (0.549)\tAcc@1 84.50 (84.50)\tAcc@5 99.00 (99.00)\n",
      " * Acc@1 84.170 Acc@5 98.880\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "train_steps = 10\n",
    "\n",
    "config = {\n",
    "            \"device\": device,\n",
    "            \"input_info\": {\n",
    "                \"sample_size\": [1, 3, 32, 32],\n",
    "            },\n",
    "            \"checkpoint_save_dir\": OUTPUT_DIR,\n",
    "            \"bootstrapNAS\": {\n",
    "                \"training\": {\n",
    "                    \"batchnorm_adaptation\": {\n",
    "                        \"num_bn_adaptation_samples\": 2\n",
    "                    },\n",
    "                    \"schedule\": {\n",
    "                        \"list_stage_descriptions\": [\n",
    "                            {\"train_dims\": [\"depth\"], \"epochs\": 5, \"init_lr\": 2.5e-6},\n",
    "                        ]\n",
    "                    },\n",
    "                    \"elasticity\": {\n",
    "                        \"available_elasticity_dims\": [\"width\", \"depth\"]\n",
    "                    },\n",
    "                    \"depth\": {\n",
    "                        \"mode\": \"manual\",\n",
    "                        \"skipped_blocks\": [\n",
    "                            [\n",
    "                                \"MobileNetV2/Sequential[features]/InvertedResidual[11]/Sequential[conv]/NNCFConv2d[2]/conv2d_0\",\n",
    "                                \"MobileNetV2/Sequential[features]/InvertedResidual[12]/__add___0\"\n",
    "                            ],\n",
    "                            [\n",
    "                                \"MobileNetV2/Sequential[features]/InvertedResidual[14]/Sequential[conv]/NNCFConv2d[2]/conv2d_0\",\n",
    "                                \"MobileNetV2/Sequential[features]/InvertedResidual[15]/__add___0\"\n",
    "                            ]\n",
    "                        ],\n",
    "                    }\n",
    "                },\n",
    "                \"search\": {\n",
    "                    \"algorithm\": \"NSGA2\",\n",
    "                    \"num_evals\": 5, #30,\n",
    "                    \"population\": 2, # 5,\n",
    "                    \"ref_acc\": model_top1_acc.item(),\n",
    "                    \"acc_delta\": 4\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "# define optimizer\n",
    "init_lr = 3e-4\n",
    "compression_lr = init_lr / 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=compression_lr)\n",
    "\n",
    "# Setup\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    nncf_config = NNCFConfig.from_dict(config)\n",
    "\n",
    "    bn_adapt_args = BNAdaptationInitArgs(data_loader=wrap_dataloader_for_init(train_loader), device=device)\n",
    "    nncf_config.register_extra_structs([bn_adapt_args])\n",
    "\n",
    "    nncf_network = create_nncf_network(model, nncf_config)\n",
    "\n",
    "\n",
    "    # Training\n",
    "    def train_epoch_fn(loader, model, compression_ctrl, epoch, optimizer):\n",
    "        train_epoch(loader, model, device, criterion, optimizer, epoch, compression_ctrl, train_iters=train_steps)\n",
    "\n",
    "    training_algorithm = EpochBasedTrainingAlgorithm.from_config(nncf_network, nncf_config)\n",
    "    nncf_network, elasticity_ctrl = training_algorithm.run(train_epoch_fn, train_loader,\n",
    "                                                           validate, val_loader, optimizer,\n",
    "                                                           OUTPUT_DIR, None,\n",
    "                                                           train_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for sub-networks\n",
    "Use NSGA2 Search algorithm to obtain the best sub-network\n",
    "Once the super-network has been trained, use NSGA2 (as specified in configuration) to obtain the best sub-network to satisfy the user's requirements. The configuration of the discovered sub-network(s) and ther performance metrics are returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 0/10]\tTime 0.569 (0.569)\tLoss 0.549 (0.549)\tAcc@1 84.50 (84.50)\tAcc@5 99.00 (99.00)\n",
      " * Acc@1 84.170 Acc@5 98.880\n",
      "Test: [ 0/10]\tTime 0.531 (0.531)\tLoss 0.268 (0.268)\tAcc@1 93.20 (93.20)\tAcc@5 99.70 (99.70)\n",
      " * Acc@1 92.990 Acc@5 99.760\n",
      "Test: [ 0/10]\tTime 0.485 (0.485)\tLoss 0.634 (0.634)\tAcc@1 80.30 (80.30)\tAcc@5 99.10 (99.10)\n",
      " * Acc@1 81.240 Acc@5 99.030\n",
      "=======================================================\n",
      "n_gen |  n_eval |  n_nds  |     eps      |  indicator  \n",
      "=======================================================\n",
      "    1 |       2 |       2 |            - |            -\n",
      "Test: [ 0/10]\tTime 0.496 (0.496)\tLoss 0.553 (0.553)\tAcc@1 83.40 (83.40)\tAcc@5 99.10 (99.10)\n",
      " * Acc@1 83.690 Acc@5 99.160\n",
      "Test: [ 0/10]\tTime 0.559 (0.559)\tLoss 0.258 (0.258)\tAcc@1 92.90 (92.90)\tAcc@5 99.90 (99.90)\n",
      " * Acc@1 92.970 Acc@5 99.840\n",
      "    2 |       4 |       2 |  0.00000E+00 |            f\n",
      "Best config: OrderedDict([(<ElasticityDim.DEPTH: 'depth'>, [9])])\n",
      "Performance metrics: [82.923008, 92.97000122070312]\n"
     ]
    }
   ],
   "source": [
    "search_algo = SearchAlgorithm.from_config(nncf_network, elasticity_ctrl, nncf_config)\n",
    "\n",
    "def validate_model_fn_top1(model, val_loader):\n",
    "    top1, _, _ = validate(model, val_loader, criterion)\n",
    "    return top1.item()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_fn_top1, val_loader,\n",
    "                                                                        OUTPUT_DIR,\n",
    "                                                                        tensorboard_writer=None)\n",
    "\n",
    "print(\"Best config: {best_config}\".format(best_config=best_config))\n",
    "print(\"Performance metrics: {performance_metrics}\".format(performance_metrics=performance_metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the search stage\n",
    "After the search has concluded, we can visualize the search progression phase as a PNG file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_algo.visualize_search_progression(filename=Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_search\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we have used just a few epochs to train the super-network. Please refer to the `schedule` section in the configuration file. Increasing the number of training epochs and the elasticity of the super-newtwork will result in a greater search space from which we can extract efficient sub-networks. For instance, this is the result that we get by increasing the number of epochs and the elasticity of the super-network: \n",
    "\n",
    "\n",
    "<img src=\"https://github.com/jpablomch/openvino_notebooks/raw/bootstrapNAS_IntelON/notebooks/303-pytorch-bootstrapnas/mobilenet_v2_bootstrapnas.png\" alt=\"BootstrapNAS MobileNetV2\" width=\"400\" align=\"left\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated in the figure, many sub-networks are more efficient than the original pre-trained model without or with little drop in accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "K5HPrY_d-7cV",
    "E01dMaR2_AFL",
    "qMnYsGo9_MA8",
    "L0tH9KdwtHhV"
   ],
   "name": "NNCF Quantization PyTorch Demo (tiny-imagenet/resnet-18)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "test_wheel",
   "language": "python",
   "name": "test_wheel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5654e4040bf9c32f9079373e7e073bea8a90b10cc5c7486a6257e4930005de1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}