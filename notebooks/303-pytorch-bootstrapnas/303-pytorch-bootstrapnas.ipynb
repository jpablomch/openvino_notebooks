{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "git68adWeq4l"
   },
   "source": [
    "# Automated Neural Architecture Search with BootstrapNAS\n",
    "\n",
    "This notebook demonstrates how to use [BootstrapNAS](https://arxiv.org/abs/2112.10878), a capability in NNCF to generate weight-sharing super-networks from pre-trained models. Once the super-network has been generated, BootstrapNAS can train it and search for efficient sub-networks. \n",
    "\n",
    "We will use [MobileNet-V2](https://arxiv.org/abs/1801.04381) pre-trained with CIFAR-10. MobileNet-V2 is an efficient mobile architecture based on inverted residual blocks. Our goal is to discover alternative models, a.k.a., subnetworks, that perform better than the input pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M1xndNu-z_2"
   },
   "source": [
    "## Imports and Settings\n",
    "\n",
    "Import NNCF and all auxiliary packages from your Python code.\n",
    "Set a name for the model, and the image width and height that will be used for the network. Also define paths where PyTorch, ONNX and OpenVINO IR versions of the models will be stored. \n",
    "\n",
    "> NOTE: All NNCF logging messages below ERROR level (INFO and WARNING) are disabled to simplify the tutorial. For production use, it is recommended to enable logging, by removing ```set_log_level(logging.ERROR)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtaM_i2mEB0z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings  # to disable warnings on export to ONNX\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import nncf  # Important - should be imported directly after torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from bootstrapnas_utils import MobileNetV2\n",
    "\n",
    "from nncf.common.utils.logger import set_log_level\n",
    "set_log_level(logging.ERROR)  # Disables all NNCF info and warning messages\n",
    "\n",
    "from nncf import NNCFConfig\n",
    "from nncf.config.structures import BNAdaptationInitArgs\n",
    "from nncf.experimental.torch.nas.bootstrapNAS import EpochBasedTrainingAlgorithm\n",
    "from nncf.experimental.torch.nas.bootstrapNAS import SearchAlgorithm\n",
    "from nncf.torch import create_compressed_model, register_default_init_args\n",
    "from nncf.torch.initialization import wrap_dataloader_for_init\n",
    "from nncf.torch.model_creation import create_nncf_network\n",
    "\n",
    "from openvino.runtime import Core\n",
    "from torch.jit import TracerWarning\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "MODEL_DIR = Path(\"model\")\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "DATA_DIR = Path(\"data\")\n",
    "BASE_MODEL_NAME = \"mobilenet-V2\"\n",
    "image_size = 32\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Paths where models will be stored\n",
    "fp32_pth_path = Path(MODEL_DIR / (BASE_MODEL_NAME + \"_fp32\")).with_suffix(\".pth\")\n",
    "model_onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME )).with_suffix(\".onnx\")\n",
    "supernet_onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_supernet\")).with_suffix(\".onnx\")\n",
    "subnet_onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_subnet\")).with_suffix(\".onnx\")\n",
    "\n",
    "# It's possible to train FP32 model from scratch, but it might be slow. So the pre-trained weights are downloaded by default.\n",
    "pretrained_on_cifar10 = True\n",
    "fp32_pth_url = \"http://hsw1.jf.intel.com/share/bootstrapNAS/checkpoints/cifar10/mobilenet_v2.pt\"\n",
    "download_file(fp32_pth_url, directory=MODEL_DIR, filename=fp32_pth_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIo5S145S0Ug",
    "outputId": "9a2db892-eb38-4863-dfdb-560aa12c8232",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Download CIFAR-10 dataset\n",
    "* 60000 images of shape 3x32x32\n",
    "* 10 different classes: airplane, automobile, etc. 6000 images per class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HxsU71bEbLS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = DATA_DIR / \"cifar10\"\n",
    "\n",
    "image_size = 32\n",
    "size = int(image_size / 0.875)\n",
    "normalize = transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                                         std=(0.2471, 0.2435, 0.2616))\n",
    "list_val_transforms = [\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ]\n",
    "val_transform = transforms.Compose(list_val_transforms)\n",
    "\n",
    "list_train_transforms = [\n",
    "            transforms.RandomCrop(image_size, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ]\n",
    "    \n",
    "train_transform = transforms.Compose(list_train_transforms)\n",
    "    \n",
    "download = False \n",
    "if not DATASET_DIR.exists(): \n",
    "    download = True\n",
    "\n",
    "train_dataset = datasets.CIFAR10(DATASET_DIR, train=True, transform=train_transform, download=download)\n",
    "val_dataset = datasets.CIFAR10(DATASET_DIR, train=False, transform=val_transform, download=download)\n",
    "\n",
    "batch_size_val = 1000\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "pin_memory = device != 'cpu'\n",
    "val_sampler = torch.utils.data.SequentialSampler(val_dataset) \n",
    "train_sampler = None\n",
    "train_shuffle = None\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size_val, shuffle=False,\n",
    "        num_workers=workers, pin_memory=pin_memory,\n",
    "        sampler=val_sampler, drop_last=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=train_shuffle,\n",
    "            num_workers=workers, pin_memory=pin_memory, sampler=train_sampler, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZX2GAh3W7ZT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- ## Pre-train Floating-Point Model\n",
    "Using NNCF for model compression assumes that the user has a pre-trained model and a training pipeline.\n",
    "\n",
    "Here we demonstrate one possible training pipeline: a ResNet-18 model pre-trained on 1000 classes from ImageNet is fine-tuned with 200 classes from Tiny-Imagenet. \n",
    "\n",
    "Subsequently, the training and validation functions will be reused as is for quantization-aware training.\n",
    " -->\n",
    " \n",
    " ## Super-network training pipeline\n",
    " \n",
    "Using NNCF for model compression assumes that the user has a pre-trained model and a training pipeline. Next, we demonstrate one possible training pipeline:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E01dMaR2_AFL"
   },
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "940rcAIyiXml"
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, model, criterion, optimizer, epoch, compression_ctrl, train_iters=None):\n",
    "    batch_time = AverageMeter(\"Time\", \":3.3f\")\n",
    "    losses = AverageMeter(\"Loss\", \":2.3f\")\n",
    "    top1 = AverageMeter(\"Acc@1\", \":2.2f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \":2.2f\")\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader), [batch_time, losses, top1, top5], prefix=\"Epoch:[{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    compression_scheduler = compression_ctrl.scheduler\n",
    "\n",
    "    if train_iters is None:\n",
    "        train_iters = len(train_loader)\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "\n",
    "        compression_scheduler.step()\n",
    "\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do opt step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        print_frequency = 50\n",
    "        if i % print_frequency == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "        if i >= train_iters:\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoNr8qwm_El2"
   },
   "source": [
    "### Validate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgnugrWgicWC"
   },
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion=nn.CrossEntropyLoss()):\n",
    "    batch_time = AverageMeter(\"Time\", \":3.3f\")\n",
    "    losses = AverageMeter(\"Loss\", \":2.3f\")\n",
    "    top1 = AverageMeter(\"Acc@1\", \":2.2f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \":2.2f\")\n",
    "    progress = ProgressMeter(len(val_loader), [batch_time, losses, top1, top5], prefix=\"Test: \")\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            print_frequency = 10\n",
    "            if i % print_frequency == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        print(\" * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\".format(top1=top1, top5=top5))\n",
    "    return top1.avg, top5.avg, losses.val "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMnYsGo9_MA8"
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R724tbxcidQE"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=\":f\"):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print(\"\\t\".join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = \"{:\" + str(num_digits) + \"d}\"\n",
    "        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Super-network from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = MobileNetV2()\n",
    "state_dict = torch.load(fp32_pth_path)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Test exporting original model to ONNX\n",
    "dummy_input = torch.randn(1, 3, image_size, image_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_top1_acc, _, _ = validate(model, val_loader, criterion) \n",
    "\n",
    "train_steps = 10\n",
    "\n",
    "config = {\n",
    "            \"device\": device,\n",
    "            \"input_info\": {\n",
    "                \"sample_size\": [1, 3, 32, 32],\n",
    "            },\n",
    "            \"checkpoint_save_dir\": OUTPUT_DIR,\n",
    "            \"bootstrapNAS\": {\n",
    "                \"training\": {\n",
    "                    # \"algorithm\": \"progressive_shrinking\",\n",
    "                    \"batchnorm_adaptation\": {\n",
    "                        \"num_bn_adaptation_samples\": 2\n",
    "                    },\n",
    "                    \"schedule\": {\n",
    "                        \"list_stage_descriptions\": [\n",
    "                            {\"train_dims\": [\"depth\"], \"epochs\": 1},\n",
    "                            # {\"train_dims\": [\"depth\"], \"epochs\": 1, \"depth_indicator\": 2},\n",
    "                            # {\"train_dims\": [\"depth\", \"width\"], \"epochs\": 1, \"depth_indicator\": 2, \"reorg_weights\": True, \"width_indicator\": 2}\n",
    "                        ]\n",
    "                    },\n",
    "                    \"elasticity\": {\n",
    "                        \"available_elasticity_dims\": [\"width\", \"depth\"]\n",
    "                    }\n",
    "                },\n",
    "                \"search\": {\n",
    "                    \"algorithm\": \"NSGA2\",\n",
    "                    \"num_evals\": 2, #30,\n",
    "                    \"population\": 1, # 5,\n",
    "                    \"ref_acc\": model_top1_acc.item(),\n",
    "                    \"acc_delta\": 4\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "def train_epoch_fn(loader, model, compression_ctrl, epoch, optimizer):\n",
    "    train_epoch(loader, model, criterion, optimizer, epoch, compression_ctrl, train_iters=train_steps)\n",
    "\n",
    "# define optimizer\n",
    "init_lr = 3e-4\n",
    "compression_lr = init_lr / 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=compression_lr)\n",
    "\n",
    "nncf_config = NNCFConfig.from_dict(config)\n",
    "\n",
    "bn_adapt_args = BNAdaptationInitArgs(data_loader=wrap_dataloader_for_init(train_loader), device=device)\n",
    "nncf_config.register_extra_structs([bn_adapt_args])\n",
    "\n",
    "nncf_network = create_nncf_network(model, nncf_config)\n",
    "\n",
    "training_algorithm = EpochBasedTrainingAlgorithm.from_config(nncf_network, nncf_config)\n",
    "\n",
    "\n",
    "nncf_network, elasticity_ctrl = training_algorithm.run(train_epoch_fn, train_loader,\n",
    "                                                       validate, val_loader, optimizer,\n",
    "                                                       OUTPUT_DIR, None,\n",
    "                                                       train_steps)\n",
    "\n",
    "search_algo = SearchAlgorithm.from_config(nncf_network, elasticity_ctrl, nncf_config)\n",
    "\n",
    "def validate_model_fn_top1(model, val_loader):\n",
    "    top1, _, _ = validate(model, val_loader, criterion)\n",
    "    return top1.item()\n",
    "\n",
    "elasticity_ctrl, best_config, performance_metrics = search_algo.run(validate_model_fn_top1, val_loader,\n",
    "                                                                    OUTPUT_DIR,\n",
    "                                                                    tensorboard_writer=None)\n",
    "\n",
    "print(\"Best config: {best_config}\".format(best_config=best_config))\n",
    "print(\"Performance metrics: {performance_metrics}\".format(performance_metrics=performance_metrics))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the search stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_algo.visualize_search_progression(filename=Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_search\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "ie.get_property(device_name=\"CPU\", name=\"FULL_DEVICE_NAME\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "K5HPrY_d-7cV",
    "E01dMaR2_AFL",
    "qMnYsGo9_MA8",
    "L0tH9KdwtHhV"
   ],
   "name": "NNCF Quantization PyTorch Demo (tiny-imagenet/resnet-18)",
   "provenance": []
  },
  "interpreter": {
   "hash": "ad096d41ce62d0546972f735df1f6703e48555b6f0c8c66be057b7b62b3afbaa"
  },
  "kernelspec": {
   "display_name": "test_wheel",
   "language": "python",
   "name": "test_wheel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}